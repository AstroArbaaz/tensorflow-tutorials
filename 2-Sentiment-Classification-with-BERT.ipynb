{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-of-the-Art Sentiment Classification in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous tutorial ([1-Construct-TensorFlow-Data-Pipeline.ipynb](https://github.com/ralphbrooks/tensorflow-tutorials/blob/master/1-Construct-TensorFlow-Data-Pipeline.ipynb)), I showed how you can create a data pipeline for a TensorFlow model. Once you have this data pipeline prepared, you are now ready to ingest this data and create a state-of-the-art classifier of emotion in language (a sentiment classifier). \n",
    "\n",
    "To this this we are going to make use of the following technologies:\n",
    "\n",
    "1) <b>Transfer Learning:</b> In 2019, Transfer Learning showed that it could achieve good performance on relatively low amounts of data. The concept is that a model is trained on a large amount of data beforehand, and this \"pre-trained\" model is then used in a second stage for classification. \n",
    "\n",
    "2) [<b>Google BERT:</b>](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) This is model that was pretrained by Google off of information from Wikipedia. It attempts to predict a word in a sentence given the words that PRECEDE the missing word and the words that FOLLOW the missing word. The model combines this prediction task with a [Transformer model architecture](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) which builds a neural network that looks at the emphasis of each word in relation to all other words in the sentence (a concept called self-attention).\n",
    "\n",
    "3) [<b>Huggingface Transformers:</b>](https://github.com/huggingface/transformers) - A startup company called Huggingface related in 2019 \"wrapper code\" that simplies using BERT as part of a data pipeline. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get started by importing in the GPU version of Tensorflow and the Transformers library from Huggingface.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* This exercise is going to require running this code with a GPU. I ran this code in AWS Sagemaker using a ml.p3.2xlarge GPU. Keep in mind that this GPU (as of Nov. 2019) costs a little\n",
    "over $4 / hour to operate.\n",
    "\n",
    "* Said differently, make sure that you stop or terminate your AWS notebook after completing this example so that you do not incur unnecessary cloud compute fees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 22.6MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-19.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this exercise involves training a model, it is important that you use the GPU version of tensorflow for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling tensorflow-1.14.0:\n",
      "  Would remove:\n",
      "    /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/freeze_graph\n",
      "    /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/saved_model_cli\n",
      "    /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/tensorboard\n",
      "    /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/tf_upgrade_v2\n",
      "    /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/tflite_convert\n",
      "    /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/toco\n",
      "    /home/ec2-user/anaconda3/envs/tensorflow_p36/bin/toco_from_protos\n",
      "    /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow-1.14.0.dist-info/*\n",
      "    /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/*\n",
      "Proceed (y/n)?   Successfully uninstalled tensorflow-1.14.0\n",
      "yes: standard output: Broken pipe\n",
      "yes: write error\n",
      "Collecting tensorflow-gpu==2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
      "\u001b[K     |████████████████████████████████| 380.8MB 26kB/s s eta 0:00:01     |██████████████████▌             | 220.4MB 37.8MB/s eta 0:00:05     |██████████████████████▉         | 271.0MB 48.4MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.16.4)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (3.7.1)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 46.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.31.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.7.1)\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/9e/a48cd34dd7b672ffc227b566f7d16d63c62c58b542d54efa45848c395dd4/tensorboard-2.0.1-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 32.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 14.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.10.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.11.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (41.0.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/81/d1e7d9974ba7c886f6d133a8baae18cb8d92b2d09bcc4f46328306825de0/google_auth-1.7.0-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 15.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.14.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.4.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 58.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<3.2,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.7)\n",
      "Collecting oauthlib>=3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 60.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.20.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.6.16)\n",
      "Building wheels for collected packages: opt-einsum\n",
      "  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opt-einsum: filename=opt_einsum-3.1.0-cp36-none-any.whl size=60860 sha256=e008998009c7460dfd6e51350be3933c97bb614362f3fd4530928350e6e238fd\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "Successfully built opt-einsum\n",
      "\u001b[31mERROR: tensorboard 2.0.1 has requirement grpcio>=1.24.3, but you'll have grpcio 1.10.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorflow-estimator, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, opt-einsum, tensorflow-gpu\n",
      "  Found existing installation: tensorflow-estimator 1.14.0\n",
      "    Uninstalling tensorflow-estimator-1.14.0:\n",
      "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
      "  Found existing installation: tensorboard 1.14.0\n",
      "    Uninstalling tensorboard-1.14.0:\n",
      "      Successfully uninstalled tensorboard-1.14.0\n",
      "Successfully installed cachetools-3.1.1 google-auth-1.7.0 google-auth-oauthlib-0.4.1 oauthlib-3.1.0 opt-einsum-3.1.0 pyasn1-modules-0.2.7 requests-oauthlib-1.3.0 tensorboard-2.0.1 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!yes | pip uninstall tensorflow\n",
    "! pip install tensorflow-gpu==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1111 21:42:32.544695 139735363929920 file_utils.py:32] TensorFlow version 2.0.0 available.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, glue_convert_examples_to_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create tf.data.Dataset from TFRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous tutorial, we had put together the encoded data as TFRecords. Now, we will set up a transformation pipeline with a ```tf.data.Dataset``` .\n",
    "\n",
    "A ```tf.data.Dataset``` allows you to construct the pipeline without having to ingest and process all of the data in each step before examining the next transformation step. This is massively critical if you are testing out your pipeline. \n",
    "\n",
    "Said differently, the last thing in the world that you want to do is to wait 3-4 minutes during an iteration of code because you are waiting for a stage of preprocessing to complete. \n",
    "\n",
    "&nbsp;  \n",
    "    \n",
    "The following code extracts the data that will be used to train the model (the tr_ds or train dataset), the data that will be used to validate the accuracy of the model (val_ds) and the holdout set of data that is used for the final verification of model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds = tf.data.TFRecordDataset(\"data/yelp_train.tfrecord\")\n",
    "val_ds = tf.data.TFRecordDataset(\"data/yelp_validate.tfrecord\")\n",
    "test_ds = tf.data.TFRecordDataset(\"data/yelp_test.tfrecord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will now extract tensors (the basic building blocks in TensorFlow) from the TFRecord. In essence, these tensors have the potential to be used as inputs into the model. \n",
    "\n",
    "Notes: \n",
    "\n",
    "* When you are extracting features from the TFRecord Dataset, it is important to note that <b>the keys in the feature spec must match the keys that that were used in the TFRecord encoding process </b> (the process that was covered in the previous tutorial). Also note that you don't have to use all of the tensors that are encoded within TFRecord (you could use just a subset of the Tensors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the encoded string tensor into the separate tensors that will feed into the model\n",
    "\n",
    "feature_spec = {\n",
    "    'idx': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'sentence': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64)\n",
    "}\n",
    "\n",
    "def parse_example(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_spec)\n",
    "\n",
    "tr_parse_ds = tr_ds.map(parse_example)\n",
    "val_parse_ds = val_ds.map(parse_example)\n",
    "test_parse_ds = test_ds.map(parse_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to cleaning up a pipeline is to map a function to the dataset. In this way, the function gets applied to each example. The following code uses this approach to clean up the sentence tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_yelp_string(features):\n",
    "    revised_sentence = tf.strings.regex_replace(features['sentence'], \"\\.\\.\\.\", \"\", replace_global=True)\n",
    "    revised_sentence = tf.strings.regex_replace(revised_sentence, \"\\\\'\", \"'\", replace_global=True)\n",
    "    revised_sentence = tf.strings.regex_replace(revised_sentence, \"\\\\n\", \"\", replace_global=True)\n",
    "    features['sentence'] = revised_sentence\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_clean_ds = tr_parse_ds.map(lambda features: clean_yelp_string(features))\n",
    "val_clean_ds = val_parse_ds.map(lambda features: clean_yelp_string(features))\n",
    "test_clean_ds = test_parse_ds.map(lambda features: clean_yelp_string(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent research( [https://arxiv.org/abs/1804.07612](https://arxiv.org/abs/1804.07612)) suggests that smaller batch training improves generalization. Based on this, I set the batch size for training at 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "EVAL_BATCH_SIZE = BATCH_SIZE * 2\n",
    "\n",
    "# XLA is the optimizing compiler for machine learning\n",
    "# It can potentially increase speed by 15% with no source code changes\n",
    "USE_XLA = False\n",
    "\n",
    "# mixed precision results on https://github.com/huggingface/transformers/tree/master/examples\n",
    "# Mixed precision can help to speed up training time\n",
    "USE_AMP = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to pull in the lengths of the different datasets from the JSON file that we created in the last tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(738, 82, 82)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Steps is determined by the number of examples\n",
    "import json\n",
    "\n",
    "with open('data/yelp_info.json') as json_file:\n",
    "    data_info = json.load(json_file)\n",
    "    \n",
    "train_examples = data_info['train_length']\n",
    "valid_examples = data_info['validation_length']\n",
    "test_examples = data_info['test_length']\n",
    "\n",
    "train_examples, valid_examples, test_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.optimizer.set_jit(USE_XLA)\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": USE_AMP})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a pipeline setup, we need to start the process of converting words into numbers so that they can be processed by the BERT transfer learning backbone. This process is commonly called Tokenization, and Huggingface includes a tokenizer that helps with this process.\n",
    "\n",
    "The tokenizers are based on the underlying research code. For example, the following are different BERT models that can be utilized within the BERT framework:\n",
    "\n",
    "    * ``bert-base-uncased``: 12-layer, 768-hidden, 12-heads, 110M parameters\n",
    "    * ``bert-large-uncased``: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
    "    * ``bert-base-cased``: 12-layer, 768-hidden, 12-heads , 110M parameters\n",
    "    * ``bert-large-cased``: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
    "    \n",
    "As seen above, the different numbers have different levels of complexity and are associated either with uncapitalized text (uncased) or text that has capitalization. I selected the bert-base-cased underlying model because the Yelp utterances have capitalization. I also selected it because in general models that are less complex tend to run faster than models which are more complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1111 21:42:53.423637 139735363929920 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpaoidsjxi\n",
      "100%|██████████| 213450/213450 [00:00<00:00, 978875.01B/s]\n",
      "I1111 21:42:53.970472 139735363929920 file_utils.py:309] copying /tmp/tmpaoidsjxi to cache at /home/ec2-user/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I1111 21:42:53.972203 139735363929920 file_utils.py:313] creating metadata file for /home/ec2-user/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I1111 21:42:53.973246 139735363929920 file_utils.py:322] removing temp file /tmp/tmpaoidsjxi\n",
      "I1111 21:42:53.973972 139735363929920 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/ec2-user/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformers framework can use a configuration dictionary in order to set up the hyperparameters for the model. In this case, I explictly use the config to make sure that the model\n",
    "is looking at num_labels=2. If we had been going through an example with three categories ('Positive', 'Negative', and 'Neutral') as opposed to just two cases ('Positive' and 'Negative') then we would have wanted to use num_labels=3 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use of the following config parameters which are stored in bert_config.json.\n",
    "\n",
    "# {\n",
    "#   \"attention_probs_dropout_prob\": 0.1,\n",
    "#   \"hidden_act\": \"gelu\",\n",
    "#   \"hidden_dropout_prob\": 0.1,\n",
    "#   \"hidden_size\": 768,\n",
    "#   \"initializer_range\": 0.02,\n",
    "#   \"intermediate_size\": 3072,\n",
    "#   \"layer_norm_eps\": 1e-12,\n",
    "#   \"max_position_embeddings\": 512,\n",
    "#   \"num_attention_heads\": 12,\n",
    "#   \"num_hidden_layers\": 12,\n",
    "#   \"num_labels\": 2,\n",
    "#   \"output_attentions\": false,\n",
    "#   \"output_hidden_states\": false,\n",
    "#   \"output_past\": true,\n",
    "#   \"pruned_heads\": {},\n",
    "#   \"torchscript\": false,\n",
    "#   \"type_vocab_size\": 2,\n",
    "#   \"use_bfloat16\": false,\n",
    "#   \"vocab_size\": 28996\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.configuration_bert import BertConfig\n",
    "config = BertConfig(\"bert_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow uses layers of abstraction when putting together a model. Operations (such as matrix algebra) can occur at a low level, and an abstraction of a neural network layer can occur at a higher level. One of these higher levels of abstraction is called a Keras model, and Huggingface uses this model as a way to abstract some of the granular details involved in using BERT for transfer learning.\n",
    "\n",
    "The following is the syntax to instantiate a keras model with the Huggingface framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1111 21:42:54.523327 139735363929920 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-tf_model.h5 not found in cache or force_download set to True, downloading to /tmp/tmpprqoym6o\n",
      "100%|██████████| 526681800/526681800 [00:23<00:00, 22244750.33B/s]\n",
      "I1111 21:43:18.652600 139735363929920 file_utils.py:309] copying /tmp/tmpprqoym6o to cache at /home/ec2-user/.cache/torch/transformers/54c7ddffbd4d74d7592fac0cca93a6542cfee71482507625c277ddd4c6b7d41c.908e74db1113031d6827eb22808cf370b0aeded6e6ac20d0f07af0a334e195cc.h5\n",
      "I1111 21:43:19.183894 139735363929920 file_utils.py:313] creating metadata file for /home/ec2-user/.cache/torch/transformers/54c7ddffbd4d74d7592fac0cca93a6542cfee71482507625c277ddd4c6b7d41c.908e74db1113031d6827eb22808cf370b0aeded6e6ac20d0f07af0a334e195cc.h5\n",
      "I1111 21:43:19.185171 139735363929920 file_utils.py:322] removing temp file /tmp/tmpprqoym6o\n",
      "I1111 21:43:19.265163 139735363929920 modeling_tf_utils.py:258] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-tf_model.h5 from cache at /home/ec2-user/.cache/torch/transformers/54c7ddffbd4d74d7592fac0cca93a6542cfee71482507625c277ddd4c6b7d41c.908e74db1113031d6827eb22808cf370b0aeded6e6ac20d0f07af0a334e195cc.h5\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have a tokenizer and the model with the right configurations, we need to take our parsed tensors (the tr_parse_ds or train parsed dataset) and feed them into the Huggingface framework. To do this, we are going to make a slight modification to the glue_convert_examples_to_features code found in the\n",
    "HuggingFace transformers repo. Here we are going to use the sst-2 task (the Stanford Sentiment Treebank binary classification task) because this task also works with binary classification. Because our example uses '1' for 'Negative' and '3' for Positive, we need to make this explicit by including the label_list keyword when doing the conversion. \n",
    "\n",
    "\n",
    "Notes:\n",
    "* Huggingface uses the similar strategy of taking the TFExamples and using a dataset in order to convert the \"sentence\" and \"labels\" into inputs that are needed by BERT (inputs such as 'input_ids', 'attention_mask', and 'token_type_ids'). As disscussed earlier in the workbook, this transformation process makes it quick to test out the conversion on a couple of data points and to move onto the next step without waiting for the full conversion to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1111 21:43:26.696814 139735363929920 glue.py:73] Using output mode classification for task sst-2\n",
      "I1111 21:43:26.807705 139735363929920 glue.py:80] Writing example 0\n",
      "I1111 21:43:26.809409 139735363929920 glue.py:119] *** Example ***\n",
      "I1111 21:43:26.810100 139735363929920 glue.py:120] guid: 367\n",
      "I1111 21:43:26.810755 139735363929920 glue.py:121] input_ids: 101 1188 1110 170 6431 7054 1708 2984 1115 4252 18389 1116 1107 8132 1555 106 1109 2618 117 1155 1103 1236 1205 1106 1103 2546 1132 1632 1120 15196 5793 119 8774 2490 1107 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.811455 139735363929920 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.812109 139735363929920 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.812753 139735363929920 glue.py:124] label: 3 (id = 1)\n",
      "I1111 21:43:26.814578 139735363929920 glue.py:119] *** Example ***\n",
      "I1111 21:43:26.815175 139735363929920 glue.py:120] guid: 17\n",
      "I1111 21:43:26.815823 139735363929920 glue.py:121] input_ids: 101 1109 13681 1303 1132 1177 3505 1105 14739 119 146 1156 2108 1176 1106 6239 162 5242 119 1124 1108 21359 14791 21361 1165 146 1814 1107 1139 16048 1115 1125 1136 1151 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.816509 139735363929920 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.817186 139735363929920 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.817832 139735363929920 glue.py:124] label: 3 (id = 1)\n",
      "I1111 21:43:26.819669 139735363929920 glue.py:119] *** Example ***\n",
      "I1111 21:43:26.820307 139735363929920 glue.py:120] guid: 368\n",
      "I1111 21:43:26.820869 139735363929920 glue.py:121] input_ids: 101 1247 3093 1106 1129 1126 15569 1104 2765 2179 4822 22850 1146 10634 1107 1142 1704 1298 119 15782 1110 1113 1103 2655 1104 1367 1105 3177 12934 11114 117 157 118 5093 1105 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.821605 139735363929920 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.822253 139735363929920 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.822880 139735363929920 glue.py:124] label: 3 (id = 1)\n",
      "I1111 21:43:26.824651 139735363929920 glue.py:119] *** Example ***\n",
      "I1111 21:43:26.825296 139735363929920 glue.py:120] guid: 394\n",
      "I1111 21:43:26.825942 139735363929920 glue.py:121] input_ids: 101 8762 3505 1234 1105 1632 1555 119 1220 1209 2222 1106 1494 1107 1251 1236 1936 119 146 1341 7352 1132 1383 1118 15782 1177 1112 1677 1112 10928 8927 1109 3945 1110 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.826602 139735363929920 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.827262 139735363929920 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.827900 139735363929920 glue.py:124] label: 3 (id = 1)\n",
      "I1111 21:43:26.829680 139735363929920 glue.py:119] *** Example ***\n",
      "I1111 21:43:26.830535 139735363929920 glue.py:120] guid: 290\n",
      "I1111 21:43:26.831206 139735363929920 glue.py:121] input_ids: 101 9800 27788 2984 117 146 2045 1107 1106 1243 1199 7352 1111 1266 125 2197 1105 1108 2288 1213 1107 1103 1166 20087 1181 2984 1105 1125 1106 2647 1149 1272 1139 1257 1127 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.831891 139735363929920 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.832534 139735363929920 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:26.833182 139735363929920 glue.py:124] label: 1 (id = 0)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = glue_convert_examples_to_features(examples=tr_clean_ds, tokenizer=tokenizer\n",
    "                                                  , max_length=128, task='sst-2'\n",
    "                                                  , label_list =['1', '3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeate the same transformation that you did for the training set on the information that will be used to validate the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1111 21:43:27.591511 139735363929920 glue.py:73] Using output mode classification for task sst-2\n",
      "I1111 21:43:27.688185 139735363929920 glue.py:80] Writing example 0\n",
      "I1111 21:43:27.689889 139735363929920 glue.py:119] *** Example ***\n",
      "I1111 21:43:27.690478 139735363929920 glue.py:120] guid: 16\n",
      "I1111 21:43:27.691060 139735363929920 glue.py:121] input_ids: 101 142 3190 10954 23955 11680 1942 8132 4125 119 17244 5578 117 1589 1304 1662 1106 2222 1106 10820 1412 2492 1114 141 18172 8231 1942 1794 119 3350 1110 1304 3044 1895 117 20122 2165 1105 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.691619 139735363929920 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.692213 139735363929920 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.692728 139735363929920 glue.py:124] label: 3 (id = 1)\n",
      "I1111 21:43:27.695237 139735363929920 glue.py:119] *** Example ***\n",
      "I1111 21:43:27.695907 139735363929920 glue.py:120] guid: 816\n",
      "I1111 21:43:27.697314 139735363929920 glue.py:121] input_ids: 101 9999 1283 1121 1142 2984 1191 1128 1328 1363 8132 1555 106 1422 2252 111 146 1338 1154 12764 1412 11947 119 1135 1882 1176 1103 7725 113 2059 1117 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.698077 139735363929920 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.698892 139735363929920 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.699551 139735363929920 glue.py:124] label: 1 (id = 0)\n",
      "I1111 21:43:27.702257 139735363929920 glue.py:119] *** Example ***\n",
      "I1111 21:43:27.702886 139735363929920 glue.py:120] guid: 568\n",
      "I1111 21:43:27.705007 139735363929920 glue.py:121] input_ids: 101 1327 1110 2488 1114 1142 2984 136 106 1398 1133 11580 3740 1138 1151 182 14824 2316 106 106 1124 2426 5133 1176 170 2618 1431 117 1235 1119 1850 1143 1106 1103 1842 1120 111 189 1106 1243 1139 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.705726 139735363929920 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.706418 139735363929920 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.708034 139735363929920 glue.py:124] label: 1 (id = 0)\n",
      "I1111 21:43:27.710801 139735363929920 glue.py:119] *** Example ***\n",
      "I1111 21:43:27.711674 139735363929920 glue.py:120] guid: 610\n",
      "I1111 21:43:27.712336 139735363929920 glue.py:121] input_ids: 101 130 119 1542 2240 112 1396 2097 1139 2072 5953 2549 2613 1111 1141 1104 1240 1178 123 13681 1107 1103 1459 1198 1106 2653 1139 4550 119 3725 9216 10424 119 1542 3048 9014 146 112 182 1254 2613 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.712986 139735363929920 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.713644 139735363929920 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.714584 139735363929920 glue.py:124] label: 1 (id = 0)\n",
      "I1111 21:43:27.718980 139735363929920 glue.py:119] *** Example ***\n",
      "I1111 21:43:27.719812 139735363929920 glue.py:120] guid: 194\n",
      "I1111 21:43:27.720938 139735363929920 glue.py:121] input_ids: 101 1332 1107 2052 1106 1267 1164 2033 170 2420 12764 1106 1139 7210 119 1753 1178 1127 1152 1682 1106 1202 1122 117 1122 112 188 1178 25522 109 1275 1167 170 2370 119 13227 1105 4931 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.721635 139735363929920 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.722743 139735363929920 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1111 21:43:27.723357 139735363929920 glue.py:124] label: 3 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = glue_convert_examples_to_features(examples=val_clean_ds, tokenizer=tokenizer\n",
    "                                                  , max_length=128, task='sst-2'\n",
    "                                                  , label_list =['1', '3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantage of using the dataset approach is that you can specify the size of your batches, and you can shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(train_examples).batch(BATCH_SIZE).repeat(-1)\n",
    "\n",
    "valid_dataset = valid_dataset.batch(EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section, we need to configure the loss function, the optimizer, and any additional metrics that we want to capture.\n",
    "\n",
    "<b>Loss:</b> \n",
    "\n",
    "This is the objective that the model is trying to minimize. In our example, the HuggingFace framework converts our Negatives ('1') and our Positives ('3') into '0' and '1' and compares this against the distribution of predicted classes. Said differently, we are trying to compare how similar the actual distribution is to the predicted distribution, and this is captured in the loss function called ```SparseCategoricalCrossentropy``` . \n",
    "\n",
    "A good discussion on cross entropy can be found at [The Gradient](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)\n",
    "\n",
    "<b>Optimizer:</b>\n",
    "\n",
    "Deep learning is the process of minimizing a loss function. The process that determines what steps to try out in each iteration is commonly referred to as the optimizer. For this exercise, I used the [Adam optimization algorithm](https://arxiv.org/pdf/1412.6980.pdf) which tends to work well in a variety of situations. \n",
    "\n",
    "<b> Metric: </b> \n",
    "\n",
    "As a basic metric, we should look at the number of times that the actual class is identical  to what is predicted. \n",
    "\n",
    "Unfortunately ,the model currently generates unscaled outputs for each example (an unscaled output for the negative class and another unscaled output for the positive class). Said differently, the model generates outputs before they are converted into probabilities (the conversion happens with a softmax function). Because of all of this, the appropriate metric to use would be ```SparseCategoricalAccuracy```.\n",
    "\n",
    "&nbsp;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "\n",
    "if USE_AMP:\n",
    "    # loss scaling is currently required when using mixed precision\n",
    "    opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, 'dynamic')\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorFlow documentation states the following:\n",
    "\n",
    "<b>If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted.</b>\n",
    "\n",
    "Because these datasets can be a precursor to training on TFRecords of significant size, it is best practice to specificially state the number of steps that will be processed per epoch in the train and validation stage. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = train_examples//BATCH_SIZE\n",
    "valid_steps = valid_examples//EVAL_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs run up to 27x faster that CPUs for model training. Because of this, it is critical that the following preconditions are in place:\n",
    "\n",
    "* You are using the version of Tensflow for GPUs\n",
    "* The code has access to a GPU\n",
    "\n",
    "To confirm the preconditions, I run the following code to detect GPUs and to see the physical devices that are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# GPU USAGE\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 23 steps, validate for 1 steps\n",
      "Epoch 1/3\n",
      "23/23 [==============================] - 33s 1s/step - loss: 0.5825 - accuracy: 0.7418 - val_loss: 0.3751 - val_accuracy: 0.8750\n",
      "Epoch 2/3\n",
      "23/23 [==============================] - 7s 289ms/step - loss: 0.3176 - accuracy: 0.8810 - val_loss: 0.5090 - val_accuracy: 0.7344\n",
      "Epoch 3/3\n",
      "23/23 [==============================] - 7s 289ms/step - loss: 0.2096 - accuracy: 0.9320 - val_loss: 0.2417 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=3, steps_per_epoch=train_steps,\n",
    "                    validation_data=valid_dataset, validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above output shows the following \n",
    "\n",
    "* In the first epoch, the train accuracy is 74% and the validation accuracy is 87.5%. This suggests that the model is underfitting. \n",
    "\n",
    "* In the second epoch, I start to see overfitting (and this carries into the third epoch). Normally, the solution to an overfitting model would be to add additional data, but the purpose of this notebook is to show high accuracy when there is a limited amount of training data. \n",
    "\n",
    "\n",
    "At this point, you see that the model has accuracy of 90%. This is fairly good considering that the Yelp rating captures the full sentiment of the user, but that in many cases, the Yelp API is only giving you part of the actual text review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the results of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above seem to look good. At this point, it is worth doing one final check before calling it a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1107 17:40:46.695626 140320940463936 glue.py:73] Using output mode classification for task sst-2\n",
      "I1107 17:40:46.712742 140320940463936 glue.py:80] Writing example 0\n",
      "I1107 17:40:46.714177 140320940463936 glue.py:119] *** Example ***\n",
      "I1107 17:40:46.714807 140320940463936 glue.py:120] guid: 293\n",
      "I1107 17:40:46.715442 140320940463936 glue.py:121] input_ids: 101 146 1108 1189 1317 4437 4423 1103 107 20315 112 188 27502 107 1788 117 6142 1128 112 1231 1682 1106 2222 1149 1103 5197 1111 1489 1552 1196 18147 1106 170 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.716065 140320940463936 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.716658 140320940463936 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.717250 140320940463936 glue.py:124] label: 1 (id = 0)\n",
      "I1107 17:40:46.719112 140320940463936 glue.py:119] *** Example ***\n",
      "I1107 17:40:46.719934 140320940463936 glue.py:120] guid: 96\n",
      "I1107 17:40:46.720581 140320940463936 glue.py:121] input_ids: 101 146 4631 146 9025 1492 119 122 119 124 1105 1408 2033 170 6994 8729 7353 119 146 1355 1106 1142 159 9866 9515 2984 1272 1142 1110 1187 146 3306 1139 2179 119 7302 1577 112 189 8239 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.721276 140320940463936 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.722006 140320940463936 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.722690 140320940463936 glue.py:124] label: 3 (id = 1)\n",
      "I1107 17:40:46.724676 140320940463936 glue.py:119] *** Example ***\n",
      "I1107 17:40:46.725318 140320940463936 glue.py:120] guid: 929\n",
      "I1107 17:40:46.726051 140320940463936 glue.py:121] input_ids: 101 146 1156 1176 1106 1474 1115 1412 1159 2097 1107 1103 1653 8897 2984 8128 1114 14675 1108 7310 1195 1400 1175 1112 1770 1112 1103 2984 1533 1105 1108 1149 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.726770 140320940463936 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.727468 140320940463936 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.728158 140320940463936 glue.py:124] label: 3 (id = 1)\n",
      "I1107 17:40:46.730349 140320940463936 glue.py:119] *** Example ***\n",
      "I1107 17:40:46.730988 140320940463936 glue.py:120] guid: 804\n",
      "I1107 17:40:46.731733 140320940463936 glue.py:121] input_ids: 101 8835 1282 1106 1838 170 1207 16759 106 7510 1108 2299 1190 1103 1141 146 1486 1113 7210 1133 1136 1115 1277 1177 1122 1445 112 189 170 1992 2239 106 1135 1108 170 2113 1263 1133 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.732342 140320940463936 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.732952 140320940463936 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.733489 140320940463936 glue.py:124] label: 3 (id = 1)\n",
      "I1107 17:40:46.735366 140320940463936 glue.py:119] *** Example ***\n",
      "I1107 17:40:46.735947 140320940463936 glue.py:120] guid: 167\n",
      "I1107 17:40:46.736565 140320940463936 glue.py:121] input_ids: 101 146 2045 1154 1103 2984 117 1185 1141 11196 1143 1176 1451 1168 13020 111 157 2984 119 6518 1261 1139 1271 1133 1225 1720 1114 1122 117 1160 1168 3813 13681 1455 1143 1191 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.737191 140320940463936 glue.py:122] attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.737873 140320940463936 glue.py:123] token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1107 17:40:46.738399 140320940463936 glue.py:124] label: 1 (id = 0)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = glue_convert_examples_to_features(examples=test_clean_ds, tokenizer=tokenizer\n",
    "                                                  , max_length=128, task='sst-2'\n",
    "                                                  , label_list =['1', '3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.batch(EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 191ms/step - loss: 0.3048 - accuracy: 0.8537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3048120141029358, 0.85365856]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the model has 84% accuracy in the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create a Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;  \n",
    "\n",
    "We can also visualize the evaluation in terms of how many true positives, true negatives, false positives, and false negatives occur. This visualization is commonly called a Confusion Matrix.\n",
    "\n",
    "Creation of the confusion matrix involves the following steps:\n",
    "\n",
    "1) Take the unnormalized outputs from the model (the logits) and compress them into probabilities (that by definition are between 0 and 1). The function that converts logits into probabilities is the softmax function.\n",
    "\n",
    "2) Once you have probabilities for each prediction, identify predicted emotion ('Negative' or 'Positive') by selecting the probability with the largest value. This is accomplished with the argmax function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(model.predict(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=48755, shape=(10, 2), dtype=float32, numpy=\n",
       "array([[0.9743283 , 0.02567171],\n",
       "       [0.7790352 , 0.22096482],\n",
       "       [0.02340363, 0.97659636],\n",
       "       [0.09775554, 0.90224445],\n",
       "       [0.963593  , 0.03640702],\n",
       "       [0.7898381 , 0.21016194],\n",
       "       [0.96113175, 0.03886823],\n",
       "       [0.38373765, 0.6162624 ],\n",
       "       [0.01492541, 0.98507464],\n",
       "       [0.17768177, 0.8223182 ]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_argmax = tf.math.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=48761, shape=(10,), dtype=int64, numpy=array([0, 0, 1, 1, 0, 0, 0, 1, 1, 1])>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_argmax[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Use a simple for loop in order to pull the true values from the test dataset. \n",
    "\n",
    "Be aware though that the Huggingface framework converted the labels that we had ('1' for negative and '3' for positive) into '0' for negative and '1' for positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.Variable([], dtype=tf.int64)\n",
    "\n",
    "for features, label in test_dataset.take(-1):\n",
    "    y_true = tf.concat([y_true, label], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=48792, shape=(30,), dtype=int64, numpy=\n",
       "array([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)  Use tf.math.confusion.matrix in order to determine true positives, true negatives, false positives, and false negatives from the true labels (y_true) and the predictions (y_pred_argmax). \n",
    "\n",
    "5) Use seaborn and matplotlib to visualize the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_confusion_matrix(y_pred_argmax, y_true):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_pred_arg: This is an array with values that are 0 or 1\n",
    "    :param y_true: This is an array with values that are 0 or 1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    cm = tf.math.confusion_matrix(y_true, y_pred_argmax).numpy()\n",
    "    con_mat_df = pd.DataFrame(cm)\n",
    "\n",
    "    sns.heatmap(con_mat_df, annot=True, fmt='g', cmap=plt.cm.Blues)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_confusion_matrix(y_pred_argmax, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the above shows the following:\n",
    "\n",
    "* The analysis of the above shows that if the customer is expressing negative sentiment. Then the classifier gets it right 91% of the time. \n",
    "* If the model is asked to evaluate positive sentiment, then the model gets it right 66% of the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create the Saved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Congratulations. You made it to the end of the tutorial and now you have a process that takes you from building a data pipeline to having an accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for the first time in 10 years, I am back on the job market looking for consulting opportunities or full time employment.  If you think I can be of help to you, feel free to reach out. I am on twitter at [@ralphbrooks](https://twitter.com/ralphbrooks) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
